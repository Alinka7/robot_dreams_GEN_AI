{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e32uEqXDInZt",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47491d3a-9ab0-49b2-d6b5-0fa1c7a1db18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets\n",
        "! pip install rouge_score\n",
        "! pip install evaluate\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import userdata\n",
        "import nltk\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
        "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "token = userdata.get(\"HF_TOKEN\")\n",
        "login('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')"
      ],
      "metadata": {
        "id": "vbTeLEsKIoYL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_KcK3qfrPp_",
        "outputId": "0e052dba-1246-4a73-f67a-94e09da5fa05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Ifa8DhrRXZCT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(\"wiki_qa\")"
      ],
      "metadata": {
        "id": "NCRdFFZL6Ksd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['train'] = df['train'].select(range(10000))\n",
        "df['validation'] = df['validation'].select(range(1000))\n",
        "df['test'] = df['test'].select(range(1000))"
      ],
      "metadata": {
        "id": "FE5-pvspTEQo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['test'] = df['test'].remove_columns(['label', 'document_title', 'question_id'])\n",
        "df['validation'] = df['validation'].remove_columns(['label', 'document_title', 'question_id'])\n",
        "df['train'] = df['train'].remove_columns(['label', 'document_title', 'question_id'])"
      ],
      "metadata": {
        "id": "bpfYDZUIMn_n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    dataset = dataset.map(lambda example: {\n",
        "        'question': remove_punctuation(example['question']),\n",
        "        'answer': remove_punctuation(example['answer']),\n",
        "    })\n",
        "    return dataset\n",
        "\n",
        "processed_df = preprocess_dataset(df)\n",
        "\n",
        "prefix = \"Please answer this question: \"\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"answer\"],\n",
        "                       max_length=512,\n",
        "                       truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_df = processed_df.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "-D0K-Qtz7Agp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\", quiet=True)\n",
        "metric_rouge = evaluate.load(\"rouge\")\n",
        "metric_bleu = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "id": "OC3UTj3GuHsN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result_rouge = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result_bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        **result_rouge,\n",
        "        **result_bleu\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "_bZqfCGh8GXz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./results\",\n",
        "   eval_strategy=\"epoch\",\n",
        "   learning_rate=0.0001,\n",
        "   per_device_train_batch_size=8,\n",
        "   per_device_eval_batch_size=4,\n",
        "   weight_decay=0.01,\n",
        "   save_total_limit=5,\n",
        "   num_train_epochs=7,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vPcfoC2Xpumv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOcd2YEpV5ie",
        "outputId": "97a053b3-415e-487d-a41c-108aa1728cfc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_df[\"train\"],\n",
        "   eval_dataset=tokenized_df[\"test\"],\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "3ttiDgqCp0r-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "0jrNtrPfSdo3",
        "outputId": "442aae5b-10d5-4b73-9a87-bbac9792f107"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malinkamyronets66\u001b[0m (\u001b[33malinkamyronets66-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241219_200323-6vtfnbgv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alinkamyronets66-/huggingface/runs/6vtfnbgv' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/alinkamyronets66-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alinkamyronets66-/huggingface' target=\"_blank\">https://wandb.ai/alinkamyronets66-/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alinkamyronets66-/huggingface/runs/6vtfnbgv' target=\"_blank\">https://wandb.ai/alinkamyronets66-/huggingface/runs/6vtfnbgv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8750' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8750/8750 37:00, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Precisions</th>\n",
              "      <th>Brevity Penalty</th>\n",
              "      <th>Length Ratio</th>\n",
              "      <th>Translation Length</th>\n",
              "      <th>Reference Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.547400</td>\n",
              "      <td>3.340540</td>\n",
              "      <td>0.204125</td>\n",
              "      <td>0.037309</td>\n",
              "      <td>0.170449</td>\n",
              "      <td>0.170372</td>\n",
              "      <td>0.013856</td>\n",
              "      <td>[0.22392573317392223, 0.03737045162266435, 0.00941157214174646, 0.0036545146626259027]</td>\n",
              "      <td>0.598206</td>\n",
              "      <td>0.660581</td>\n",
              "      <td>14219</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.271100</td>\n",
              "      <td>3.323027</td>\n",
              "      <td>0.205574</td>\n",
              "      <td>0.037617</td>\n",
              "      <td>0.169033</td>\n",
              "      <td>0.169078</td>\n",
              "      <td>0.015074</td>\n",
              "      <td>[0.22379487179487179, 0.04, 0.01005940594059406, 0.00378494623655914]</td>\n",
              "      <td>0.623881</td>\n",
              "      <td>0.679443</td>\n",
              "      <td>14625</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.064400</td>\n",
              "      <td>3.326957</td>\n",
              "      <td>0.204644</td>\n",
              "      <td>0.036629</td>\n",
              "      <td>0.166714</td>\n",
              "      <td>0.166691</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>[0.22458429804251737, 0.03976458160416509, 0.010446421284583368, 0.004087798809206434]</td>\n",
              "      <td>0.600370</td>\n",
              "      <td>0.662160</td>\n",
              "      <td>14253</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.941000</td>\n",
              "      <td>3.336313</td>\n",
              "      <td>0.205654</td>\n",
              "      <td>0.035155</td>\n",
              "      <td>0.168747</td>\n",
              "      <td>0.168767</td>\n",
              "      <td>0.014253</td>\n",
              "      <td>[0.22472378569939547, 0.0368904488089015, 0.00936163344362844, 0.003862698621718901]</td>\n",
              "      <td>0.609128</td>\n",
              "      <td>0.668571</td>\n",
              "      <td>14391</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.829800</td>\n",
              "      <td>3.357401</td>\n",
              "      <td>0.205876</td>\n",
              "      <td>0.034117</td>\n",
              "      <td>0.168047</td>\n",
              "      <td>0.168020</td>\n",
              "      <td>0.013935</td>\n",
              "      <td>[0.2243647754959972, 0.03569023569023569, 0.009138697937727456, 0.003783545974483062]</td>\n",
              "      <td>0.607481</td>\n",
              "      <td>0.667364</td>\n",
              "      <td>14365</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.760200</td>\n",
              "      <td>3.361717</td>\n",
              "      <td>0.203199</td>\n",
              "      <td>0.033669</td>\n",
              "      <td>0.166652</td>\n",
              "      <td>0.166547</td>\n",
              "      <td>0.013850</td>\n",
              "      <td>[0.22414536052858375, 0.03605694831321572, 0.009812143575981215, 0.004119370194068107]</td>\n",
              "      <td>0.579325</td>\n",
              "      <td>0.646876</td>\n",
              "      <td>13924</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.704300</td>\n",
              "      <td>3.370099</td>\n",
              "      <td>0.202716</td>\n",
              "      <td>0.033135</td>\n",
              "      <td>0.166162</td>\n",
              "      <td>0.166132</td>\n",
              "      <td>0.013403</td>\n",
              "      <td>[0.221312049289365, 0.035157720394489195, 0.008466986892452984, 0.003722414251528849]</td>\n",
              "      <td>0.602278</td>\n",
              "      <td>0.663554</td>\n",
              "      <td>14283</td>\n",
              "      <td>21525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.22392573317392223, 0.03737045162266435, 0.00941157214174646, 0.0036545146626259027]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.22379487179487179, 0.04, 0.01005940594059406, 0.00378494623655914]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.22458429804251737, 0.03976458160416509, 0.010446421284583368, 0.004087798809206434]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.22472378569939547, 0.0368904488089015, 0.00936163344362844, 0.003862698621718901]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.2243647754959972, 0.03569023569023569, 0.009138697937727456, 0.003783545974483062]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.22414536052858375, 0.03605694831321572, 0.009812143575981215, 0.004119370194068107]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Trainer is attempting to log a value of \"[0.221312049289365, 0.035157720394489195, 0.008466986892452984, 0.003722414251528849]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8750, training_loss=3.0161517926897323, metrics={'train_runtime': 2224.0762, 'train_samples_per_second': 31.474, 'train_steps_per_second': 3.934, 'total_flos': 1949771665760256.0, 'train_loss': 3.0161517926897323, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель досягла трохи кращих результатів при зменшенні learning rate, при збільшенні кількосі епох особливих покращень помічено не було, можливо варто спробувати більше епох, і тоді буде результат кращий."
      ],
      "metadata": {
        "id": "WVwnjiRh4HUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "lwa2dybAs-U3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = \"how are glacier caves formed?\"\n",
        "inputs = \"Please answer to this question: \" + my_question\n",
        "inputs = tokenizer(inputs, return_tensors=\"pt\").to('cuda')\n",
        "outputs = trainer.model.generate(**inputs)\n",
        "answer = tokenizer.decode(outputs[0])\n",
        "from textwrap import fill\n",
        "\n",
        "print(fill(answer, width=80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d367CM1stkJi",
        "outputId": "f5984121-8289-44a6-8893-d1177188787f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> The glacier caves are a type of cave in the Antarctica</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./results/checkpoint-8500\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./results/checkpoint-8500\")\n",
        "for my_question in [\"How are the directions of the velocity and force vectors related in a circular motion\",\n",
        "                    \"how much does baby ruth candy?\"]:\n",
        "  inputs = tokenizer(\"Please answer this question: \" + my_question, max_length=512,\n",
        "                        truncation=True, return_tensors='pt')\n",
        "\n",
        "  inputs = {key: value for key, value in inputs.items()}\n",
        "  outputs = model.generate(**inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  print(f\"Question: {my_question}\")\n",
        "  print(f\"Generated Answer: {answer}\")\n",
        "  print(\"-------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvcNwKFFt_7i",
        "outputId": "c746d287-976d-4b4b-f5c8-1ba6b886d21e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How are the directions of the velocity and force vectors related in a circular motion\n",
            "Generated Answer: In physics circular motion refers to the motion of a body in a circular motion\n",
            "-------------------------------\n",
            "Question: how much does baby ruth candy?\n",
            "Generated Answer: Baby Ruth is a brand name of a candy company based in New York City\n",
            "-------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Відповіді в датасеті достатньо специфічні.\n",
        "# !!!!!!!!!! example of real answer from dataset:\n",
        "\n",
        "# how much does baby ruth candy? It is owned by the Swiss company Nestlé'\n",
        "# how are pointe shoes made? The edge of the toe pad, which is inserted between the foot and toe box for cushioning, can be seen on the right foot."
      ],
      "metadata": {
        "id": "TtR0jGKxSp2G"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REAL MODEL WO FINE TUNING"
      ],
      "metadata": {
        "id": "MPSmVbDrtQc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "for example in df['test']:\n",
        "    question = example['question']\n",
        "    true_answer = example['answer']\n",
        "    inputs = tokenizer(\"Please answer this question: \" + question, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    predictions.append(generated_answer)\n",
        "    references.append([true_answer])\n",
        "\n",
        "bleu_results = metric_bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "rouge_results = metric_rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(f\"BLEU Score: {с}\")\n",
        "print(f\"ROUGE Score: {rouge_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0hRQEzStC_7",
        "outputId": "46b70740-d748-4905-9f97-ceeba1e459a6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: {'bleu': 1.322091241496486e-05, 'precisions': [0.17799009200283086, 0.03271537622682661, 0.015241320914479255, 0.014175257731958763], 'brevity_penalty': 0.00039476268906210257, 'length_ratio': 0.11315768399135101, 'translation_length': 2826, 'reference_length': 24974}\n",
            "ROUGE Score: {'rouge1': 0.043865721319756114, 'rouge2': 0.008630067146450301, 'rougeL': 0.04072683204371634, 'rougeLsum': 0.040756615994795076}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOTNblT9tDGX",
        "outputId": "d817b534-a787-405d-fffd-9b2b0e890aac"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 1.322091241496486e-05,\n",
              " 'precisions': [0.17799009200283086,\n",
              "  0.03271537622682661,\n",
              "  0.015241320914479255,\n",
              "  0.014175257731958763],\n",
              " 'brevity_penalty': 0.00039476268906210257,\n",
              " 'length_ratio': 0.11315768399135101,\n",
              " 'translation_length': 2826,\n",
              " 'reference_length': 24974}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4CzpxCQtDIn",
        "outputId": "53418551-ffa7-4112-9308-213a8bb42617"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.043865721319756114,\n",
              " 'rouge2': 0.008630067146450301,\n",
              " 'rougeL': 0.04072683204371634,\n",
              " 'rougeLsum': 0.040756615994795076}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = \"how much does baby ruth candy?\"\n",
        "inputs = tokenizer(\"Please answer this question: \" + my_question, max_length=512,\n",
        "                       truncation=True, return_tensors='pt')#, padding=True, )\n",
        "\n",
        "inputs = {key: value for key, value in inputs.items()}\n",
        "outputs = model.generate(**inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Question: {my_question}\")\n",
        "print(f\"Generated Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kciDCFg1tDLL",
        "outputId": "2b201d7d-641b-4d33-aaf8-5f00e3a941b3"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: how much does baby ruth candy?\n",
            "Generated Answer: $1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Висновки:\n",
        "1.  Fine-tuned модель має значно вищі показники по всіх ROUGE метриках, що вказує на її здатність краще генерувати тексти, які збігаються з оригінальними відповідями.\n",
        "2. Fine-tuned модель має кращі значення precision для всіх n-грам, що свідчить про її більшу точність у генеруванні відповідей.\n",
        "3.  Fine-tuned модель генерує текст з більш природною довжиною, тоді як звичайна модель генерує дуже короткі відповіді.\n",
        "4. Fine-tuned модель показує значно кращі результати в порівнянні зі звичайною моделлю за всіма основними метриками: ROUGE, BLEU, precision, brevity penalty і length ratio. Вона більш точна, генерує тексти, які краще відповідають оригіналам, і має більш природну довжину відповіді. Звичайна модель має низькі показники на всіх метриках, що свідчить про її слабку здатність генерувати відповіді, що відповідають оригінальним даним."
      ],
      "metadata": {
        "id": "CW5iOo2I2W4c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cmieaeV2Qh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Вказуємо шлях до папки, яку потрібно заархівувати\n",
        "folder_path = '/content/results/checkpoint-8500'  # Замість my_folder вкажіть вашу папку\n",
        "zip_name = '/content/results/checkpoint-8500.zip'\n",
        "\n",
        "# Архівуємо папку\n",
        "shutil.make_archive(zip_name.replace('.zip', ''), 'zip', folder_path)\n",
        "from google.colab import files\n",
        "\n",
        "# Завантажуємо zip-файл\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pNlyc44u2Qk8",
        "outputId": "dff0cc5b-3ca4-4b4f-e94b-d19f6342399f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abcc3b62-20fa-425b-9e2e-66c80fda21f5\", \"checkpoint-8500.zip\", 2637635056)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AfqwLegM6pQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}